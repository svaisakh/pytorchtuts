{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can be constructed using the ```torch.nn``` package.\n",
    "\n",
    "Now that you had a glimpse of autograd, ```nn``` depends on autograd to define models and differentiate them. An ```nn.Module``` contains layers, and a method ```forward(input)``` that returns the output.\n",
    "\n",
    "For example, look at this network that classfies digit images:\n",
    "\n",
    "![convnet](resources/Neural-Networks/mnist.png)\n",
    "\n",
    "It is a simple feed-forward network. It takes the input, feeds it through several layers one after the other, and then finally gives the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "* Define the neural network that has some learnable parameters (or weights)\n",
    "* Iterate over a dataset of inputs\n",
    "* Process input through the network\n",
    "* Compute the loss (how far is the output from being correct)\n",
    "* Propagate gradients back into the network’s parameters\n",
    "* Update the weights of the network, typically using a simple update rule:\n",
    "```python\n",
    "weight = weight - learning_rate * gradient\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let’s define this network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution kernels\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net (\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear (400 -> 120)\n",
       "  (fc2): Linear (120 -> 84)\n",
       "  (fc3): Linear (84 -> 10)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You just have to define the ```forward``` function, and the ```backward``` function (where gradients are computed) is **automatically** defined for you using autograd. You can use any of the Tensor operations in the ```forward``` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The learnable parameters of a model are returned by ```net.parameters()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, torch.Size([6, 1, 5, 5]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "len(params), params[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The input to the forward is an ```autograd.Variable```, and so is the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.0846  0.0483 -0.0866  0.0726  0.0004  0.0736 -0.1055 -0.0678  0.1393 -0.0650\n",
       "[torch.FloatTensor of size 1x10]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input = Variable(torch.randn(1, 1, 32, 32))\n",
    "out = net(_input)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Zero the gradient buffers of all parameters and backprop with random gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "_Note:_\n",
    "\n",
    "```torch.nn``` only supports _mini-batches_ The entire ```torch.nn``` package only supports inputs that are a mini-batch of samples, and **not a single sample**.\n",
    "\n",
    "For example, ```nn.Conv2d``` will take in a 4D Tensor of $(n_{Samples}, n_{Channels}, Height, Width)$.\n",
    "\n",
    "If you have a single sample, just use ```input.unsqueeze(0)``` to add a _fake_ batch dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Before proceeding further, let’s recap all the classes you’ve seen so far.\n",
    "\n",
    "\n",
    "**Recap**:\n",
    "* ```torch.Tensor``` - A multi-dimensional array.\n",
    "* ```autograd.Variable``` - Wraps a ```Tensor``` and records the _history of operations_ applied to it. Has the same API as a ```Tensor```, with some additions like ```backward()```. Also holds the _gradient_ $w.r.t.$ the ```Tensor```.\n",
    "* ```nn.Module``` - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.\n",
    "* ```nn.Parameter``` - A kind of ```Variable```, that is _automatically_ registered as a parameter when assigned as an attribute to a Module.\n",
    "* ```autograd.Function``` - Implements ```forward``` and ```backward``` definitions of an autograd operation. Every ```Variable``` operation, creates at least a single ```Function``` node, that connects to functions that created a ```Variable``` and encodes its history.\n",
    "\n",
    "\n",
    "\n",
    "**At this point, we covered**:\n",
    "\n",
    "* Defining a neural network\n",
    "* Processing inputs and calling ```backward```\n",
    "\n",
    "\n",
    "**Still Left**:\n",
    "\n",
    "* Computing the loss\n",
    "* Updating the weights of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A loss function takes the $(output, target)$ pair of inputs, and computes a value that estimates how far away the output is from the target.\n",
    "\n",
    "There are several different loss functions under the ```nn``` package . A simple loss is: ```nn.MSELoss``` which computes the _mean-squared error_ between the input and the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 38.5450\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = net(_input)\n",
    "target = Variable(torch.arange(1, 11))  # a dummy target, for example\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(out, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, if you follow loss in the backward direction, using it’s ```.grad_fn``` attribute, you will see a graph of computations that looks like this:\n",
    "\n",
    "```python\n",
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> view -> linear -> relu -> linear -> relu -> linear\n",
    "      -> MSELoss\n",
    "      -> loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So, when we call loss.backward(), the whole graph is differentiated w.r.t. the loss, and all Variables in the graph will have their .grad Variable accumulated with the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For illustration, let us follow a few steps backward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.autograd.function.MSELossBackward at 0x7fde2e827408>,\n",
       " <torch.autograd.function.AddmmBackward at 0x7fde2e827318>,\n",
       " <AccumulateGrad at 0x7fde2ebfcb00>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad_fn, loss.grad_fn.next_functions[0][0], loss.grad_fn.next_functions[0][0].next_functions[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To backpropogate the error all we have to do is to ```loss.backward()```. You need to **clear the existing gradients** though, else _gradients will be accumulated to existing gradients_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we shall call ```loss.backward()```, and have a look at _conv1_’s bias gradients before and after the backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('conv1.bias.grad before backward', Variable containing:\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       " [torch.FloatTensor of size 6])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.zero_grad()  # zeroes the gradient buffers of all parameters\n",
    "'conv1.bias.grad before backward', net.conv1.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('conv1.bias.grad after backward', Variable containing:\n",
       " 1.00000e-02 *\n",
       "   3.2736\n",
       "   3.0928\n",
       "  -0.1395\n",
       "   1.7141\n",
       "  -5.9198\n",
       "  -2.7355\n",
       " [torch.FloatTensor of size 6])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "'conv1.bias.grad after backward', net.conv1.bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, we have seen how to use loss functions.\n",
    "\n",
    "The only thing left to learn is:\n",
    "\n",
    "* updating the weights of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "_Read Later_:\n",
    "\n",
    "The neural network package contains various modules and loss functions that form the building blocks of deep neural networks. A full list with documentation is [here](http://pytorch.org/docs/nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest update rule used in practice is the _Stochastic Gradient Descent_ (SGD):\n",
    "\n",
    "```python\n",
    "weight = weight - learning_rate * gradient\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement this using simple python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as you use neural networks, you want to use various different update rules such as _SGD_, _Nesterov-SGD_, _Adam_, _RMSProp_, etc. To enable this, we built a small package: torch.optim that implements all these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using it is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), learning_rate)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()  # zero the gradient buffers\n",
    "output = net(_input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()  # Does the update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
